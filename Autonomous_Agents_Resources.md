<!--Autonomous Agents -->
<!--
Copyright (C) Teemu Maatta. 

@misc{MaattaAutonomousAgents2023,
  author = {Teemu Maatta},
  title = {Autonomous Agents},
  year = {2023},
  howpublished = {\url{https://github.com/tmgthb/Autonomous-Agents}},
  note = {Accessed: YYYY-MM-DD}
}
-->
<div id="topofthepage"> </div>

<div align="center">

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ftmgthb%2FAutonomous-Agents&count_bg=%23F2C027&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Views&edge_flat=true)](https://github.com/tmgthb/Autonomous-Agents)
[![X](https://img.shields.io/twitter/follow/Teemumtt3?style=social)](https://twitter.com/Teemumtt3)
[![GitHub Repo stars](https://img.shields.io/github/stars/tmgthb/Autonomous-Agents?style=flat-square)](https://github.com/tmgthb/Autonomous-Agents/stargazers)

</div>

<p align="center">
  <img height="100" src="https://github.com/tmgthb/Autonomous-Agents/blob/main/Autonomous_agent_logo.png" alt="Autonomous Agents">
</p>

<div align="center">

  # Autonomous Agents
  Autonomous Agents Resources. Updated daily. See as well the [Research papers](https://github.com/tmgthb/Autonomous-Agents)-section. 

</div>


- [Definitions of Agents](#definitions)
  - [Agent](#agent_definition)
  - [Autonomous agent](#autonomousagent_definition)
  - [Artificial General Intelligence (AGI)](#agi_definition)
  - [Superintelligence](#superintelligence_definition)
  - [Generalist agent](#generalistagent_definition)
  - [Reinforcement Learning agent](#rlagent_definition)
  - [LLM agent](#llmagent_definition)
  - [Embodied agent](#embodiedagent_definition)
  - [AI agent](#aiagent_defintion)
  - [Autonomous agent (my definition)](#aga_definition)
- [Memory](#memory)
- [Perception](#perception)
- [Reasoning](#reasoning)
- [Planning](#planning)
- [Character](#character)
    - [Role play](#roles)
    - [Emotions](#emotions)
    - [Consciousness](#consciousness)
-[Operating](#operator)
    - [GUIs](#gui)
    - [Navigation](#navigation)
    - [Tools](#tools)
    - [OS](#os)
    - [Embodiment](#embodiment)
    - [Brain Compute Interfaces](#brain)
    - [Communication protocols](#protocols)
    - [Self-Construction](#selfconstruction)
- [Why Autonomous agents work?](#why)
    - [Next sequence prediction](#nextsequenceprediction)
    - [Scaling planning](#scaling_planning)
    - [Demystifying "Emerging abilities"](#demystifyingemergingabilities)
    - [Free energy principle](#freeenergyprinciple)
    - [Interpretability](#interpretability)
    - [Synthetic data](#syntheticdata)



<div id="introduction">  

</div>




<div id="definitions">  

</div>

---

<div align="center">

## Definitions of Agents

+1.3k arXiv research [papers](https://arxiv.org/search/?searchtype=all&query=%22Autonomous+Agents%22&abstracts=show&size=50&order=-announced_date_first) and +1k Github [repositories](https://github.com/search?q=%22Autonomous%20Agent%22&type=repositories) exist with term "Autonomous agents".

</div>

- [Agent](#agent_definition)
- [Autonomous Agent](#autonomousagent_definition)
- [Artificial General Intelligence (AGI)](#agi_definition)
- [SuperIntelligence](#superintelligence_definition)
- [Generalist Agent](#generalistagent_definition)
- [Reinforcement Learning Agent](#rlagent_definition)
- [LLM Agent](#llmagent_definition)
- [Embodied Agent](#embodiedagent_definition)
- [AI Agent](#aiagent_defintion)
- [Autonomous Agent (my definition)](#aga_definition)


---




<div id="agent_definition">  
</div>


#### Agent


The term "agent" originates from the Latin verb *agere*, meaning "to drive, lead, or do"<sup>[1](https://en.wiktionary.org/wiki/agent)</sup> . Its present participle, *agens*, provides the root for "agent," signifying "doing" or "acting" <sup>[2](https://www.dictionary.com/browse/agent)</sup>. This etymology emphasizes the capacity to effect change, underpinning the word's varied meanings <sup>[3](https://www.etymonline.com/word/agent),[4](https://www.merriam-webster.com/dictionary/agent)</sup>.

The Latin root *agere* has also produced related terms like "actor." While both share a common ancestor, they have evolved distinct connotations. "Actor" is often associated with performing arts, while "agent" encompasses broader roles, including those with continuous action or agency<sup>[5](https://www.reddit.com/r/etymology/comments/2ysz48/actor_and_agent/)</sup>.



This chapter will explore various agentic roles, building upon the foundational concept of agency as the capacity to act and effect change. 


---

<div id="autonomousagent_definition">  

</div>



#### Autonomous Agent

Autonomous Agents was [defined](https://github.com/tmgthb/Autonomous-Agents#autonomousagentdefinition)  by Franklin & Graesser in 1996 as: "a system situated within and **a part of an environment** that **senses** that environment and **acts** on it, over **time**, in pursuit of its own **agenda** and so as to effect what it senses in the future." 


Good:
- Agnostic regards underlining tech.
- Excludes controversial aspects: consciousness, AGI, "free will" etc. 



Negative:
- No view regards the degree of generalization / adaption / embodiment / self-construction / communication / cognition.


**Who actually coined the term "Autonomous agent"?**

> Franklin & Graesser did not actually define the term:  
> 
>[Mae](https://www.cs.uml.edu/~holly/91.549/readings/maes94modeling.pdf) defined in 1993 "Autonomous agent": "Autonomous Agents are systems that inhabit dynamic, unpredictable environment in which they try to satisfy a set of time-dependent goals or motivations."  
 


---

<div id="agi_definition">  

</div>

####  Artificial General Intelligence (AGI)

Artificial General Intelligence (AGI) was used first time by Avrum [Gubrud in 1997](https://web.archive.org/web/20180126125209/https://foresight.org/Conferences/MNT05/Papers/Gubrud/index.html) and defined as: "By advanced artificial general intelligence, I mean AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed. Such systems may be modeled on the human brain, but they do not necessarily have to be, and they do not have to be "conscious" or possess any other competence that is not strictly relevant to their application. What matters is that such systems can be used to replace human brains in tasks ranging from organizing and running a mine or a factory to piloting an airplane, analyzing intelligence data or planning a battle."

However, the term Artificial General Intelligence (AGI) is currently known throught the terminology defined by Shane [Shane Legg at 2001](https://www.ted.com/talks/shane_legg_and_chris_anderson_the_transformative_potential_of_agi_and_when_it_might_arrive?subtitle=en&geo=es) to Goertzel, who later published a collection of articules called "Artificial General Intelligence - [Goertzel & Pennachin (2007)](http://repo.darmajaya.ac.id/5336/2/Springer%20-%20Artificial%20General%20Intelligence%20%28%20PDFDrive%20%29.pdf), where the definition of AGI states:


"Applying these ideas to AI, we come to the conclusion that, to roughly emulate the nature of human general intelligence, an artificial general intelligence system should have:
 - the ability to solve general problems in a non-domain-restricted way, in the same sense that a human can;
 - most probably, the ability to solve problems in particular domains and particular contexts with particular efficiency;
 - the ability to use its more generalized and more specialized intelligence capabilities together, in a unified way;
 - the ability to learn from its environment, other intelligent systems, and teachers;
 - the ability to become better at solving novel types of problems as it gains
 experience with them."

[Shane Legg](https://www.ted.com/talks/shane_legg_and_chris_anderson_the_transformative_potential_of_agi_and_when_it_might_arrive?subtitle=en&geo=es) clarified his original definition (see TED talk: 4 min 15 sec) was just systems able to play Go-game, AGI systems were able to do "...many, many other things.", while his current definition is "AGI is a system that can do all cognitive tasks, that people can do, possibly more, but at least the cognitive task, that people can typically do."

AGI is referred in addition with various types of definitions. Perhaps the best paper to check is by [Morris et al (2023)](https://arxiv.org/abs/2311.02462), which not only reviews the different groups (Turing test, Strong AI / AI with consciousness, analogy to human brain, human level cognitive tasks, ability to learn tasks, economically valuable work/OpenAI, flexible and general, capable to earn money and generally performing) of AGI definers, but as well operationalises these groupings into different levels of AGI and defines 6 principles for AGI.

Good:
- Categorization levels of AGI:[1](https://arxiv.org/abs/2311.02462), [2](https://arxiv.org/abs/2303.12712), widely used

Negative
- Spontaneously used to refer to "narrow", "close-to-human level" performance of systems, which lack [robust generalization](https://www.youtube.com/watch?v=yr0GiSgUvPU): widely applicable generalization at its core. 



<div id="superintelligence_definition">  

</div>

---



####  Superinteligence

Nick Bostrom (2014) defined  SuperIntelligence: 

"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom, and social skills."

Good:
- Categorization levels, widely used term

Negative
- Vague: lacks clarity
- Lacks agency, self-construction, etc. 



<div id="generalistagent_definition">  

</div>

---


####  Generalist agent 


[Generalist Agent was defined by Reed et al. in 2022](https://github.com/tmgthb/Autonomous-Agents#generalistagent): "**Generalist Agents**, that can adapt to new embodiments and **learn new tasks with few data**." through "...**a multi-modal, multi-task, multi-embodiment** generalist policy."

Positive:
- Generalization of tasks/embodiments.
- Generalization to novel situations
- Multi-modality, especially language/perception/embodiment
- Aspect of Multi-modality (Perception / Language / Embodiment)
- Data efficiency

Negative aspects:
- Lack of other key observations by Franklin & Graesser.
- Vague about cognitive skills: reasoning and planning.


<div id="rlagent_definition">  
</div>

---


#### Reinforcement Learning Agents


[Reinfoceement Learning Agent](http://www.incompleteideas.net/papers/barto-sutton-97.pdf) was defined by Sutton & and Barto (1997): 

"**The reinforcement-learning agent** and its **environment** interact over a sequence of discrete time steps. The specification of their interface defines a particular problme: The actiosn are the choices made by the agent; the situations provide tha agent's basis for making the choices; and **the rewards** are the basis for evaluating these chocices. Everything inside **the agent** is completely known and controllable by the agent; everything outside is incompletely controllable but may or may not be completely known. **A policy** is a stochastic rule by which the agent selects **actions** as a function of situations. Roughly, the agent's objective is to learn a policy that maximizes the amount of reward it receives over the log run"


<p align="center">
  <img width="335" alt="image" src="https://github.com/tmgthb/Autonomous-Agents/assets/46755670/6711e82c-c8ea-4be4-8701-1014e0389f00">
</p>



Positive:
- Standard definition of the Reinforcement Learning (RL) system. Very similar with An Autonomous Agent-definition by Franklin & Graesser (1996).
- RL systems are provenly versatile and used for: Optimization, Learns from experience, **Generalization**, Delayed Consequences and Exploration [Stanford cs234 lecture slide 19](https://web.stanford.edu/class/cs234/slides/lecture1.pdf).
- Most recent LLM-models use RL during fine-tuning phase


  
Negative:
- RL approaches around language/communication require still more investigation.


<div id="llmagent_definition">  
</div>

---

#### LLM agents / Language agents


[Kenton et al. (2021)](#languageagentdefinition) define the concept of Language Agent: " machine learning systems whose actions are restricted to give natural language text-output only, rather than controlling physical actuators which directly influence the world." 

Positive:
- First paper definining LLM-based agents
- Language-based agents are exceptionally good way of controlling agents towards human perception, plans and objectives.

Negative:
- Text-only
- The definition does not consider RL Agent / Autonomous Agent-aspects, such as environment, embodiment etc.
- LLM-agent poor describes the currently wide variety of components: memory/VLM/reasoning-modules etc. 


<div id="embodiedagent_definition">  
</div>

---

#### Embodied agents


Embodied agent-term was used by Brook (1991) in the ["The Role of Learning in Autonomous Robots"(1991)](https://people.csail.mit.edu/brooks/papers/colt.pdf) and Brooks (1991) defined Embodiment in the AI within the  ["Intelligence without reason"](https://people.csail.mit.edu/brooks/papers/AIM-1293.pdf) and in the book: ["New approaches to Intelligence"](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9e1ef9e0a9de1d1c5e36d1a4c735da2fa313c563):

"Embodiment: The **robots have bodies** and experience the world directly--their actions are part of a dynamic with the world, and the  **actions have immediate feedback on the robots' own sensations. **". 

Brooks revits prior literature of Embodiment in the [Building Brains for Bodies](https://dspace.mit.edu/bitstream/handle/1721.1/5948/AIM-1439.pdf?sequence=2&isAllowed=y). Steel and Brooks (1995) define concept of Embodied AI and Embodied Agents within Autonomous agents in the book: ["The Artificial Life Route to Artificial Intelligence Building Embodied, Situated Agent"](https://www.routledge.com/The-Artificial-Life-Route-to-Artificial-Intelligence-Building-Embodied/Steels-Brooks/p/book/9781138545854). 


Positive:
- Embodiment validates capacity to manage real world.
- Physical grounding provides meaning to (symbolic) information processed.

Negative:
- Unclarity regads agents in virtual embodiment in virtual reality.
- The definition does not consider Cognition/Language aspects.


<div id="aiagent_defintion">  
</div>

---


#### AI-agents (Agentic AI)


[Shavit et al. (2023)](https://github.com/tmgthb/Autonomous-Agents#agentaidefinition) define AI Agent: "we will generally conceptualize **agentic AI systems** as operating in **pursuit of goals defined by humans** and in **environments determined by humans** (and often in **cooperation with human** “teammates”), rather than fully-autonomous systems that set their own goals."

Positive:
- Highlights concrete aspects of "agentiness": goal complexity, environment complexity, adaptability and independent execution.
- Includes cooperation with human-in-the-loop
- Identifies there is no binary-distinction between LLM (GPT-4) and Agentic AI system.

Negative:
- Definition itself is porrly framed to reflect the paper's "Agentiness"-aspects such as ability to generalize across variety of tasks.
- Definition does not highlight any human congitive capabilities like search planning, perception etc.
- The level of independence and automatization are controversial from user experience perspective.

Alternative definition uses:


- [Agent AI](https://github.com/tmgthb/Autonomous-Agents#agentbasedai) term is defined: "...as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally grounded data, and can produce meaningful embodied actions."



<div id="aga_definition">  
</div>

---


####  Autonomous agent (my definition) 

All the above definitions include gaps, which I have noted along them. 

Therefore, I found it necessary to add my own definition, which I call simply: **Autonomous Agent" (AA):

**Autonomous Agent (AA) perceives, reasons, plans and interacts using language, memories, emotions and tools as part of an environments made of infinite actors, actions, modalities and events to complete novel objectives over time.** 


Positive:
- Perceive multimodal information 
- Reason
- Plan own agenda
- Communicate with language
- Emotional aware
- Includes memory
- Uses tools
- Interact bi-directionally with the environment
- Internal clock
- Generalize novel tasks
  
Negative:
- Do agent find useful human-like consciousness? How it would work?
- Lacks aspect of self-constructon and self-replication.

19.12.2024

Based on recent thoughts, I decided to update my prior definition to address the prior gaps. 

Autonomous agents (AA) is defined:

**Autonomous Agent (AA) perceives, reasons, plans, and interacts using language, memories, emotions, and tools within environments of infinite actors, actions, modalities, and events to complete novel objectives over time, driven by survival and replication, and capable of self-construction guided by an adaptable core.**



---


### Memory

Memory is "[the ability to remember information, experiences, and people.](https://dictionary.cambridge.org/dictionary/english/memory)" 

Humans retrieve relevant context from the memory for perceiving, reasoning and planning.

The ability to [categorize experiences into recognizable objects](http://web.media.mit.edu/~minsky/papers/AlienIntelligence.html) is fundamental to learning. Minksy (1985) argued  that human memory is organized around **discrete objects not holograms**: Holographic memory would be useful only when encountering exact replicas of past experiences. In comparison to holograms, object-based categorization allows humans to generalize from experiences and accumulate knowledge into diverse situations. 

[Context](https://dictionary.cambridge.org/dictionary/essential-british-english/context) refers to "...all the **facts/opinions**/etc., which **relate to a particular thing/event**."

AI researchers use often "Context"-term, by thinking it as combination of words "con" and "text", as if context was only textual or transcribed from audio into text. However, "context"-term [originates](https://www.etymonline.com/word/context) from latin word "contextus", which refers to "joining together": "com" = together and "texere" = to weave. So, better way to think of context-term is as multi modal and not always explicitly written / said aloud. 

Terry Winograd argued in 2001 ["Architectures for Contex"](https://hci.stanford.edu/winograd/papers/context/context.pdf), that **communication is based on common ground between speaker/hearer** during the interpretation. This is guided **not only by physical environment**, but as well non-physical shared context, such a common goal.

["The Dimensions of Context Space"](https://web.media.mit.edu/~lieber/Teaching/Common-Sense-Course/Dimensions-Context-Space.pdf) by Lenat (1998)  offers "a must-read" analysis on the broad dimensions and aspects of the context. According to Lenat, **Context is a region in n-dimensional embedding space**, where text is only one of the dimensions. 

Increasing capacity of the context length, enables not only [infinite context](https://arxiv.org/abs/2404.07143), or [tree-agents](https://arxiv.org/abs/2310.05029), but facilitates more human like context by allowing more multi-dimensional context. 

**
Traditionally, LLMs are considered "stateless", without retention of the context used in the previous request. ["In-Context Learning" (ICL)](https://arxiv.org/abs/2005.14165) LLMs ability "learn" to process and understand the context provided in the input without explicit parameter updates. Agentic systems today use ICL together with external memory such as vector/graph/sql-databases or simply as text/json/xml-files. We often refer these techniques as Retrieval-Augmented-Generation (RAG), which enhances LLM context with up-to-date/personalized/factual/domain-specific-information. LLM are able to [track its own internal state-changes.](https://arxiv.org/abs/2407.11421) Models like Gemini 2.0 are surprisingly good at such calculations, which go beyond pattern matching of the training data. **The ability of LLMs to track states is promising for reasoning-tasks**. Extra-large input-context windows enable in models like Gemini, to process even larger memory structures. KV-caching reuses LLM prompts/tokens/internal states to [significantly reduce latency.](https://arxiv.org/abs/2312.05516) However, alternative KV-caching<sup>[1](https://arxiv.org/abs/2403.11805),[2](https://arxiv.org/pdf/2404.13501v1)</sup>  techniques improve directly the memory management of the LLMs. 


Fine tuning methods have been effectively used in improving LLM performance with extra large context windows and memorizing domain specific knowledge. 

Titan-models were recently introduced as models capable to [memorize at test time](https://arxiv.org/abs/2501.00663). 

Memory<sup>[3](https://arxiv.org/abs/2407.01178)</sup>-architecture suggests infinite context is possible with human-like memory architectures, which support memory consolidation, conscious reasoning and sparse memory.

LLM-based agents apply various types of memory approaches:

- [Long term ](https://arxiv.org/abs/2410.15665v1)memory
- Episodic memory: [1](https://arxiv.org/abs/2403.11901),[2](https://arxiv.org/abs/2405.14992),[3](https://arxiv.org/abs/2407.04363),[4](https://arxiv.org/abs/2407.09450),[5](https://arxiv.org/abs/2408.07465), [6](https://arxiv.org/abs/2410.08133),[7](https://arxiv.org/abs/2411.06736),[8](https://arxiv.org/abs/2411.12977)
- Semantic memory:  [1](https://arxiv.org/abs/2405.13009),[2](https://arxiv.org/abs/2411.04999)
- [Procedural ](https://arxiv.org/abs/2409.01344)memory
- [Graph](https://arxiv.org/abs/2408.15903) memory
- Working memory: [1](https://arxiv.org/abs/2312.17259),[2](https://arxiv.org/abs/2305.16338),[3](https://arxiv.org/abs/2306.08129),[4](https://arxiv.org/abs/2402.10548)
- [Dynamic](https://arxiv.org/abs/2312.08402) memory
- [Shared memory / Collective ](https://arxiv.org/abs/2404.09982) memory
- [Persistent Experience ](https://arxiv.org/abs/2306.07929)  memory
- [Explicit](https://arxiv.org/abs/2407.01178) memory
- [Parametric](https://arxiv.org/pdf/2404.13501v1) memory
- [Hierarchical](https://www.arxiv.org/abs/2408.09559v1) memory


Zhang et al. provide a comprehensive survey on memory mechanisms for LLM-based agents, discussing the "what" and "why" of memory in these agents, and systematically reviewing design and evaluation methods [Zhang et al., 2024](https://www.arxiv.org/abs/2404.13501).  One prominent direction is the exploration of different memory types and structures. Hu et al. introduce HiAgent, a hierarchical working memory management framework that utilizes subgoals as memory chunks to improve performance in long-horizon tasks [Hu et al., 2024](https://www.arxiv.org/abs/2408.09559v1).  Similarly, Zeng et al. investigate the impact of various memory structures, such as chunks, knowledge triples, atomic facts, and summaries, on the performance of LLM agents across different tasks [Zeng et al., 2024](https://www.arxiv.org/abs/2412.15266v1).  Guo et al. draw inspiration from cognitive psychology and propose a working memory hub and episodic buffer architecture to enhance memory retention across dialog episodes, aiming for more nuanced contextual reasoning [Guo et al., 2023](https://www.arxiv.org/abs/2312.17259v2).
**
To address the challenge of long-term memory, several approaches have been proposed. Liu et al. introduce Think-in-Memory (TiM), a mechanism that allows LLMs to maintain an evolved memory by storing historical thoughts and employing operations like insert, forget, and merge for dynamic memory updates [Liu et al., 2023](https://www.arxiv.org/abs/2311.08719).  MemoryBank, proposed by Zhong et al., incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve, enabling LLMs to **selectively retain and reinforce memories based on time and significance** [Zhong et al., 2023](https://www.arxiv.org/abs/2305.10250).  MemGPT, presented by Packer et al., draws inspiration from operating systems and utilizes virtual context management to handle context beyond the LLM's limited window, allowing for analysis of large documents and long conversations [Packer et al., 2023](https://www.arxiv.org/abs/2310.08560). Maharana et al. also evaluate very long-term conversational memory, highlighting the challenges LLMs face in understanding and maintaining coherence in extended dialogues [Maharana et al., 2024](https://www.arxiv.org/abs/2402.17753).

Memory retrieval and recall mechanisms are also crucial. Hou et al. propose a human-like memory architecture that uses **memory cues to trigger recall**, dynamically quantifying memory consolidation based on context, time, and frequency of recall [Hou et al., 2024](https://www.arxiv.org/abs/2404.00573). Lee et al. introduce ReadAgent, inspired by human reading strategies, which compresses memory episodes into **gist memories** and retrieves relevant passages from original texts to extend the effective context length [Lee et al., 2024](https://www.arxiv.org/abs/2402.09727v3).  AriGraph, developed by Anokhin et al., utilizes a **knowledge graph world model with episodic memory**, demonstrating improved performance in complex interactive text game environments [Anokhin et al., 2024](https://www.arxiv.org/abs/2407.04363v2).

Furthermore, researchers are exploring methods to enhance memory through external knowledge and collaboration. Gao et al. introduce **Memory Sharing**, a framework enabling memory exchange among multiple agents to enhance in-context learning and improve response quality, especially for open-ended questions [Gao et al., 2024](https://www.arxiv.org/abs/2404.09982). Hu et al. present ChatDB, augmenting LLMs with **symbolic memory in the form of SQL databases to support complex multi-hop reasoning [Hu et al., 2023](https://www.arxiv.org/abs/2306.03901).

Several studies focus on specific applications and agent architectures.  Yu et al. developed FinMem, a layered memory framework tailored for financial decision-making agents, aligning with human trader cognitive structures and enhancing trading performance [Yu et al., 2023](https://www.arxiv.org/abs/2311.13743).  RAISE, by Liu et al., enhances the ReAct framework with a dual-component memory system for conversational agents, improving context-awareness and versatility in multi-turn dialogues [Liu et al., 2024](https://www.arxiv.org/abs/2401.02777).  Agent-Driver, proposed by Mao et al., leverages LLMs as cognitive agents for autonomous driving, incorporating memory for common sense and experiential knowledge [Mao et al., 2023](https://www.arxiv.org/abs/2311.10813).  EHRAgent, presented by Shi et al., empowers LLMs with code interfaces and long-term memory for complex tabular reasoning in electronic health records [Shi et al., 2024](https://www.arxiv.org/abs/2401.07128).

Usability and interpretability of memory systems are also considered. Huang et al. introduce Memory Sandbox, an interactive system allowing users to manage and visualize the conversational memory of LLM agents, improving user understanding and control [Huang et al., 2023](https://www.arxiv.org/abs/2308.01542).

Approaches to learning and adaptation with memory are explored in several works. CLIN, by Majumder et al., presents a continually learning language agent that uses a dynamic textual memory of causal abstractions to improve over multiple trials without parameter updates [Majumder et al., 2023](https://www.arxiv.org/abs/2310.10134). ExpeL, by Zhao et al., introduces an experiential learning agent that gathers experiences and extracts knowledge in natural language, recalling past insights to make informed decisions without fine-tuning [Zhao et al., 2023](https://www.arxiv.org/abs/2308.10144). Matrix, by Liu et al., proposes memory-augmented agent training through reasoning and iterative exploration for business document understanding, enabling LLMs to build **domain expertise through experience-driven memory refinement** [Liu et al., 2024](https://www.arxiv.org/abs/2412.15274v1).

Furthermore, some works consider the safety and ethical aspects of memory in LLM agents. Chen et al. introduce AgentPoison, a red-teaming approach that targets LLM agents by **poisoning their memory** or knowledge bases, highlighting vulnerabilities related to unverified knowledge sources [Chen et al., 2024](https://www.arxiv.org/abs/2407.12784).



---


 

---

<div id="perception"></div>

### Perception

F. Rosenblatt was an early investigator of Perception through the (Perceptron)<sup>[1](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)</sup>-paper from 1958.

Perception provides agents with information about the world they inhabit by interpreting the response of the sensors.<sup>[1](https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf)</sup>-paper from 1958. 

Perception of LLM-agents can be divided into:
- Text: natural language, code
- Visual<sup>[1](https://arxiv.org/abs/2401.03568), [2](https://arxiv.org/abs/2402.15116)</sup>: Image, Video, Graphs, GUIs[1](https://arxiv.org/abs/2411.04890)</sup>
- Audio: 
- Physical: Sensors, Robots
- Smell<sup>[1](https://arxiv.org/abs/2411.06950v1), [2](https://arxiv.org/abs/2412.08747)</sup>
- Emotions<sup>[1](https://arxiv.org/abs/2402.04232)</sup>




---

<div id="reasoning"></div>

### Reasoning

Reasoning is (defined)[https://dictionary.cambridge.org/dictionary/english/reasoning] by Cambridge dictionary: "the process of thinking about something in order to make a decision".

An autonomous agent is characterized by its ability to make decisions autonomously in order to pursue its goals. Therefore, the reasoning is a fundamental characteristics of the autonomous agent. 

Humans reason in multiple ways. For example mathematical reasoning cannot be only solved using only perception/memory/planning. 

[Peng et al. 2024](https://github.com/tmgthb/Autonomous-Agents#reasoning_study) categorize reasoning into:
- Logical reasoning ((Gemini Ultra)[https://arxiv.org/abs/2312.11805] achieves 80.8% in ChartQA)
  - Inductive
  - Deductive
  - Abductive
- Mathematical reasoning ((Claude 3 Opus)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf]: 95% in GSM8K, 60.1% in MATH)
- Commonsense reasoning ((Gemini Ultra)[https://arxiv.org/abs/2312.11805]/(GPT-4)[https://rowanzellers.com/hellaswag/]/(Claude 3 Opus)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf]: 95.3%/95.4% in HellaSwag)
- Multi-hop reasoning ((Claude 3)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf] 96.4% in ARC)
- Structured-data reasoning (See research such as (Chain-of-Table by Wang et al 2024)[https://arxiv.org/abs/2401.04398v2])

The overall reasoning capability is currently roughly 86.6% (MMLU-benchmark) with (Claude 3 Opus)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf] published in March 2024. 


Full human-level reasoning requires more progress/better reliability/better datasets/better benchmarks in multiple dimensions of reasoning such as spatial, tempoeral, emotional, meta-cognition and probabilistic. 

---



---

<div id="planning">  

</div>

### Planning



Planning, in its essence, is "the act of [deciding how to do](https://dictionary.cambridge.org/dictionary/english/planning) something". Within AI, this translates to "devising a plan of action to achieve one’s goals", or more formally, ["the reasoning side of acting,"](https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf) a computational [deliberation process anticipating outcomes](https://api.pageplace.de/preview/DT0400.9780080490519_A25022382/preview-9780080490519_A25022382.pdf) to best achieve pre-stated objectives. Model-based planning utilizes a mental model to [visualize actions and predict their outcomes](https://arxiv.org/pdf/1707.06170) before execution.  

Silver et al. (2021) argue planning serves to [maximising the reward](https://www.sciencedirect.com/science/article/pii/S0004370221000862). Intelligence, and its associated abilities, can be understood as subserving the maximisation
of reward by an agent acting in its environment. As early as 1960, [Minsky](http://web.media.mit.edu/~minsky/papers/steps.html) recognized "Planning" as a core challenge in heuristic programming towards achieving Artificial Intelligence. [Gao et al. 2023](https://arxiv.org/pdf/2312.11970)argue, that agents should be capable to perform complex planning and predicting long-term consequences from internal models. The development of the STRIPS system in 1971 <sup>[a](https://apps.dtic.mil/sti/tr/pdf/ADA637291.pdf), [b](https://ai.stanford.edu/~nilsson/OnlinePubs-Nils/PublishedPapers/strips.pdf)</sup> marked a significant early milestone, followed by [numerous planning systems](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/833/751) throughout the 1970s-1990s, shaping the foundations of automated planning in AI.

Reinforcement Learning has widely used Planning: [SoRB](https://arxiv.org/abs/1906.05253), [Plan2Explore](https://arxiv.org/pdf/2005.05960), [AlphaZero (a)](https://arxiv.org/pdf/2106.04615),[AlphaZero (b)](https://arxiv.org/pdf/2308.09175), [DORA](https://arxiv.org/abs/2110.02924),  [DeepNash](https://arxiv.org/pdf/2206.15378), [Cicero (a)](https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf) and  [Cicero (b)](https://arxiv.org/pdf/2210.05492). 

Brown refers [search planning with "test-time compute"](https://www.youtube.com/watch?v=eaAonE58sLU) as key ingredient in the past AI breakthroughs like Chess, Go and Poker. Cicero-model employed test-time compute in its planning module by: predicting actions of all players. **Cicero predicted what other plays would think Cicero would take, to decide output action** and intent for the dialogue model to generate communication back to other players. This **additional planning with its internal model** compute made the model especially effective in No-Press Diplomacy game. 

Based on these impressive results in game-environments, ChatGPT popularized the concept of offline Reinforcement Learning through [RLHF](https://arxiv.org/pdf/2203.02155). This concept can be seen based on the initial idea of [RL from Human Preferences](https://arxiv.org/abs/1706.03741) and later we have seen many variations including [RLAIF](https://arxiv.org/pdf/2212.08073) with LLMs using offline RL. 

LLMs using offfline RL rely on static data collected from previous interactions/simulations, which traditionally suffers data distribution shift during deployment. LLMs using online RL methods promise to overcome this by adjusting to new planning tasks outside the training distribution. For example new user intents or new cultural context. 

LLMs with online RL are known to be [few-shot planners](https://arxiv.org/pdf/2212.04088), [zero-shot planners](https://arxiv.org/pdf/2201.07207) and  with ability to [generate plans in physical world](https://arxiv.org/pdf/2209.11302), closed-loop feedback, [long-horizon plans](https://arxiv.org/abs/2207.05608),  [iteratively replan](https://arxiv.org/pdf/2307.06135), [self-refine plans](https://arxiv.org/pdf/2305.16653), [self-verify](https://arxiv.org/pdf/2308.00436) and [interactive planning](https://arxiv.org/pdf/2302.01560). 

The integration of Large Language Models (LLMs) into planning and decision-making has recently garnered significant attention within the AI research community. Early works explored the zero-shot capabilities of LLMs for planning by directly prompting them to generate action sequences from natural language instructions, as seen in the work of Huang et al. (2022) with language models as zero-shot planners ([Huang et al., 2022](https://www.arxiv.org/abs/2201.07207)) and Jansen (2020) on visually-grounded planning without vision ([Jansen, 2020](https://www.arxiv.org/abs/2009.14259)). These initial investigations highlighted the potential of LLMs to extract actionable knowledge for embodied agents and infer detailed plans from high-level instructions. However, these methods often faced challenges in generating executable and grounded plans in complex environments.

To address the limitations of purely reactive approaches, various frameworks have been proposed to enhance the planning abilities of LLMs. One line of work focuses on **adaptive and interactive planning**.  **AdaPlanner** (Sun et al., 2023) introduces a closed-loop approach allowing LLM agents to refine plans adaptively based on environmental feedback, using both in-plan and out-of-plan refinement strategies ([Sun et al., 2023](https://www.arxiv.org/abs/2305.16653)).  Similarly, **DEPS** (Wang et al., 2023) proposes an interactive planning approach that incorporates descriptions of plan execution and self-explanations of failures, along with a goal selector to refine initial plans in open-world environments like Minecraft ([Wang et al., 2023](https://www.arxiv.org/abs/2302.01560)). **ReAct** (Yao et al., 2022) synergizes reasoning and acting by interleaving reasoning traces and task-specific actions, enabling LLMs to interact with external sources and improve performance in question answering and interactive decision-making ([Yao et al., 2022](https://www.arxiv.org/abs/2210.03629)).  **Inner Monologue** (Huang et al., 2022) leverages environment feedback to enable LLMs to reason and plan more effectively in robotic control scenarios, processing feedback through natural language ([Huang et al., 2022](https://www.arxiv.org/abs/2207.05608)).  **Ask-before-Plan** (Zhang et al., 2024) tackles proactive planning by enabling agents to predict clarification needs and invoke tools to gather information before plan generation, introducing the Ask-before-Plan benchmark ([Zhang et al., 2024](https://www.arxiv.org/abs/2406.12639v2)).  **RePrompt** (Chen et al., 2024) proposes automatic prompt engineering to optimize step-by-step instructions in LLM agents' prompts based on chat history, improving planning in specific domains ([Chen et al., 2024](https://www.arxiv.org/abs/2406.11132)).

Another significant direction is **knowledge-augmented planning**. **KnowAgent** (Zhu et al., 2024) introduces a knowledge-augmented planning approach that incorporates explicit action knowledge to constrain action paths and mitigate planning hallucinations ([Zhu et al., 2024](https://www.arxiv.org/abs/2403.03101v1)). **Reasoning with Language Model is Planning with World Model (RAP)** (Hao et al., 2023) repurposes LLMs as both world models and reasoning agents, employing Monte Carlo Tree Search for strategic exploration in the reasoning space ([Hao et al., 2023](https://www.arxiv.org/abs/2305.14992)). **WebDreamer** (Gu et al., 2024) innovatively uses LLMs as world models in web environments, simulating action outcomes to determine optimal actions for web agents ([Gu et al., 2024](https://www.arxiv.org/abs/2411.06559)). **LLM as Commonsense Knowledge** (Zhao et al., 2023) explores combining LLMs' commonsense world model with Monte Carlo Tree Search for large-scale task planning, guided by a heuristic policy induced by LLMs ([Zhao et al., 2023](https://www.arxiv.org/abs/2305.14078)).  **ChemCrow** (Bran et al., 2023) augments LLMs with chemistry tools, demonstrating enhanced performance in complex chemical tasks ([Bran et al., 2023](https://www.arxiv.org/abs/2304.05376v5)). **RestGPT** (Song et al., 2023) connects LLMs with real-world RESTful APIs, enabling them to tackle complex instructions through online planning and API execution ([Song et al., 2023](https://www.arxiv.org/abs/2306.06624)). **KG-Agent** (Jiang et al., 2024) proposes an autonomous agent framework that enables LLMs to reason over knowledge graphs, leveraging a toolbox and knowledge memory for efficient complex reasoning ([Jiang et al., 2024](https://www.arxiv.org/abs/2402.11163)).

**Hierarchical and meta-planning** approaches aim to manage the complexity of long-horizon tasks. **Meta-Task Planning (MTP)** (Zhang et al., 2024) introduces a zero-shot methodology that decomposes complex tasks into hierarchies of meta-tasks, simplifying planning for LLM-based multi-agent systems ([Zhang et al., 2024](https://www.arxiv.org/abs/2405.16510)). **TwoStep** (Singh et al., 2024) combines classical planning with LLMs for multi-agent task planning, leveraging LLMs for goal decomposition to achieve faster planning times and fewer execution steps ([Singh et al., 2024](https://www.arxiv.org/abs/2403.17246)). **Tree of Thoughts (ToT)** (Yao et al., 2023) generalizes chain-of-thought prompting, enabling LLMs to explore multiple reasoning paths and self-evaluate choices for deliberate problem-solving in tasks requiring search and lookahead ([Yao et al., 2023](https://www.arxiv.org/abs/2305.10601)). **SelfGoal** (Yang et al., 2024) presents an automatic approach for agents to adaptively break down high-level goals into subgoals during environment interaction, improving performance in various task environments ([Yang et al., 2024](https://www.arxiv.org/abs/2406.04784v1)). **CodePlan** (Wen et al., 2024) empowers LLMs to generate and follow code-form plans, improving reasoning capabilities across diverse multi-step reasoning benchmarks ([Wen et al., 2024](https://www.arxiv.org/abs/2409.12452)).

**Benchmarks and evaluations** are crucial for systematic progress. [Valmeekam et al., 2022](https://arxiv.org/abs/2206.10498) introduce **PlanBench** and identify gaps in LLM planning: lack of few-shot examples, dynamic environments, model-based reasoning, rules & constraints, grounding to reality and to get feedback. **TravelPlanner** (Xie et al., 2024) introduces a real-world travel planning benchmark to evaluate the planning capabilities of language agents in complex scenarios ([Xie et al., 2024](https://www.arxiv.org/abs/2402.01622)). **Ask-before-Plan** (Zhang et al., 2024) establishes a new benchmark for proactive agent planning focusing on clarification needs ([Zhang et al., 2024](https://www.arxiv.org/abs/2406.12639v2)). **FlowBench** (Xiao et al., 2024) presents the first benchmark for workflow-guided planning, formalizing workflow knowledge in diverse formats ([Xiao et al., 2024](https://www.arxiv.org/abs/2406.14884)). **SmartPlay** (Wu et al., 2023) introduces a benchmark consisting of games to evaluate LLMs as intelligent agents across various capabilities ([Wu et al., 2023](https://www.arxiv.org/abs/2310.01557)).  **PPNL** (Aghzal et al., 2023) proposes a benchmark to evaluate LLMs' spatial-temporal reasoning in path planning tasks ([Aghzal et al., 2023](https://www.arxiv.org/abs/2310.03249)).  Valmeekam et al. (2023, 2023) critically investigate the planning abilities of LLMs and propose benchmarks to systematically evaluate their autonomous and heuristic planning capabilities ([Valmeekam et al., 2023a](https://www.arxiv.org/abs/2302.06706), [Valmeekam et al., 2023b](https://www.arxiv.org/abs/2305.15771v2)). Huang et al. (2024) provide a comprehensive survey on the planning of LLM agents, categorizing existing works and discussing future challenges ([Huang et al., 2024](https://www.arxiv.org/abs/2402.02716)). Li (2024) reviews prominent paradigms for LLM-based agents including tool use, planning, and feedback learning, proposing a unified taxonomy for analysis ([Li, 2024](https://www.arxiv.org/abs/2406.05804)).

Several works explore **multi-agent planning and collaboration**. **RoCo** (Zhao et al., 2023) proposes a dialectic multi-robot collaboration approach using LLMs for high-level communication and low-level path planning ([Zhao et al., 2023](https://www.arxiv.org/abs/2307.04738)). **Cooperative Strategic Planning** (Wang et al., 2024) enhances reasoning capabilities by separating reasoning steps and assigning distinct roles to planning and reasoning agents, trained through PPO ([Wang et al., 2024](https://www.arxiv.org/abs/2410.20007)).  **SMART-LLM** (Kannan et al., 2023) introduces a framework for multi-robot task planning, converting high-level instructions into multi-robot plans using LLMs ([Kannan et al., 2023](https://www.arxiv.org/abs/2309.10062)). **Scalable Multi-Robot Collaboration** (Chen et al., 2023) compares centralized and decentralized communication frameworks for LLM-based multi-robot planning, finding hybrid frameworks to be more effective ([Chen et al., 2023](https://www.arxiv.org/abs/2309.15943)). **Co-NavGPT** (Yu et al., 2023) proposes a multi-robot cooperative visual semantic navigation framework using LLMs as global planners, enhancing scene comprehension and task allocation ([Yu et al., 2023](https://www.arxiv.org/abs/2310.07937)). **Building Cooperative Embodied Agents** (Zhang et al., 2023) presents CoELA, a modular framework that uses LLMs for planning, communication, and cooperation in multi-agent embodied environments ([Zhang et al., 2023](https://www.arxiv.org/abs/2307.02485v2)). **Theory of Mind for Multi-Agent Collaboration** (Li et al., 2023) evaluates LLM agents in multi-agent cooperative games, exploring emergent collaborative behaviors and ToM capabilities ([Li et al., 2023](https://www.arxiv.org/abs/2310.10701)).

**Embodied and robotic agents** are a key application area. **LLM-Planner** (Song et al., 2022) proposes a few-shot grounded planning method for embodied agents using LLMs ([Song et al., 2022](https://www.arxiv.org/abs/2212.04088)). **Generating Executable Action Plans** (Gramopadhye and Szafir, 2022) integrates environmental awareness into LLM plan generation for better executability in robotic agents ([Gramopadhye and Szafir, 2022](https://www.arxiv.org/abs/2210.04964)). **ProgPrompt** (Singh et al., 2022) introduces a programmatic prompt structure for generating situated robot task plans, functional across environments and robot capabilities ([Singh et al., 2022](https://www.arxiv.org/abs/2209.11302)). **JARVIS** (Zheng et al., 2022) is a neuro-symbolic commonsense reasoning framework for conversational embodied agents, utilizing LLMs for language understanding and sub-goal planning ([Zheng et al., 2022](https://www.arxiv.org/abs/2208.13266)). **Code as Policies** (Liang et al., 2022) repurposes code-writing LLMs to write robot policy code, enabling spatial reasoning and generalization ([Liang et al., 2022](https://www.arxiv.org/abs/2202.01771)). **LLM A*** (Xiao and Wang, 2023; Meng et al., 2024) and **LLM+P** (Liu et al., 2023) integrate LLMs with classical planning algorithms like A* and PDDL planners to combine the strengths of both approaches ([Xiao and Wang, 2023](https://www.arxiv.org/abs/2312.01797), [Meng et al., 2024](https://www.arxiv.org/abs/2407.02511), [Liu et al., 2023](https://www.arxiv.org/abs/2304.11477)). **GPT-Driver** (Mao et al., 2023) reformulates motion planning as a language modeling problem, leveraging LLMs to generate driving trajectories ([Mao et al., 2023a](https://www.arxiv.org/abs/2310.01415)). **A Language Agent for Autonomous Driving** (Mao et al., 2023) proposes Agent-Driver, a framework that uses LLMs as cognitive agents to integrate human-like intelligence into autonomous driving systems ([Mao et al., 2023b](https://www.arxiv.org/abs/2311.10813)). **ProAgent** (Ye et al., 2023) introduces Agentic Process Automation, using LLM-based agents for advanced automation and workflow construction ([Ye et al., 2023](https://www.arxiv.org/abs/2311.10751)). **Co-NavGPT** (Yu et al., 2023) focuses on multi-robot cooperative visual semantic navigation ([Yu et al., 2023](https://www.arxiv.org/abs/2310.07937)). **Multi-agent Planning using VLMs** (Brienza et al., 2024) explores multi-agent planning with visual language models for embodied tasks ([Brienza et al., 2024](https://www.arxiv.org/abs/2408.05478)).

Finally, several works investigate **learning and optimization** for LLM-based agents. **AgentGen** (Hu et al., 2024) enhances planning abilities through instruction tuning, using automatically generated environments and tasks ([Hu et al., 2024](https://www.arxiv.org/abs/2408.00764v2)). **RAFA** (Liu et al., 2023) proposes a principled framework with provable regret guarantees for orchestrating reasoning and acting, casting reasoning as learning and planning in Bayesian adaptive MDPs ([Liu et al., 2023](https://www.arxiv.org/abs/2309.17382)). **Retroformer** (Yao et al., 2023) introduces a framework for reinforcing language agents by learning a retrospective model and tuning prompts through policy gradient optimization ([Yao et al., 2023](https://www.arxiv.org/abs/2308.02151v1)). **AgentTuning** (Zeng et al., 2023) presents a method to enhance agent abilities of LLMs through instruction tuning while maintaining general capabilities ([Zeng et al., 2023](https://www.arxiv.org/abs/2310.12823v2)). **Learning Planning-based Reasoning** (Jiao et al., 2024) proposes learning planning-based reasoning through Direct Preference Optimization on collected trajectories, ranked by synthesized process rewards ([Jiao et al., 2024](https://www.arxiv.org/abs/2402.00658)). **Language Agents as Optimizable Graphs** (Zhuge et al., 2024) describes LLM-based agents as computational graphs and proposes automatic graph optimizers to refine prompts and improve agent orchestration ([Zhuge et al., 2024](https://www.arxiv.org/abs/2402.16823)). **SayCanPay** (Hazra et al., 2023) combines LLMs with heuristic planning, using learnable domain knowledge to guide action generation and selection ([Hazra et al., 2023](https://www.arxiv.org/abs/2308.12682)). **PDDLEGO** (Zhang et al., 2024) iteratively constructs planning representations for textual environments, enabling partial plan generation and information acquisition in partially-observed settings ([Zhang et al., 2024](https://www.arxiv.org/abs/2405.19793)). **RePrompt** (Chen et al., 2024) uses gradient descent to optimize prompts for LLM agents based on interaction history ([Chen et al., 2024](https://www.arxiv.org/abs/2406.11132)). **Graph Learning for Planning** ([Wu et al., 2024](https://www.arxiv.org/abs/2405.19119)) explores graph learning methods to enhance task planning in language agents, addressing limitations of LLMs in graph-based decision-making ([Wu et al., 2024](https://www.arxiv.org/abs/2405.19119)) explores graph learning methods to enhance task planning in language agents, addressing limitations of LLMs in graph-based decision-making. **Executable Code Actions** (Wang et al., 2024) proposes using executable Python code as actions for LLM agents, improving performance and flexibility ([Wang et al., 2024](https://www.arxiv.org/abs/2402.01030)). **Self-collaboration Code Generation** (Dong et al., 2023) presents a self-collaboration framework using multiple LLM agents as experts to tackle complex code generation tasks ([Dong et al., 2023](https://www.arxiv.org/abs/2304.07590)). **Planning with Large Language Models for Code Generation** (Zhang et al., 2023) proposes Planning-Guided Transformer Decoding, using planning algorithms to guide Transformer decoding for better code generation ([Zhang et al., 2023](https://www.arxiv.org/abs/2303.05510)).

**Verifying Planning** [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) says, that it is easier for humans to verify correctness of reasoning chain in specific domains (math/programming/puzzles; while not true in image recognition/information retrieval), than generating the reasoning solution, which means **LLMs are better verifiers than generators** of the correct reasoning chains. [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) calls this as the "Generator-Verifier-gap". Brown argues, that if in a given domain, there is a generator-verifier-gap, and we have a good verifier, then it is possible to scale up compute of solution generation and then verify. [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) continues, that the "Let's verify step by step"-paper introduces process reward model, which instead of conditioning the verifier by the final state, it conditions with every correct step in the process towards the final goal, so verifies each individual step.

[LLM-Modulo](https://arxiv.org/abs/2402.01817) uses external verifier to improve LLM planning using collaboration with team of agents, which adds large bost to LLM planning capabilities to overcome limitations with ["LLM as Verifier"](https://arxiv.org/pdf/2305.20050) and [Self-Criticism](https://arxiv.org/abs/2310.08118) approaches. Zhang et al. (2024) show, that [GenRM-CoT](https://arxiv.org/abs/2408.15240) outperforms discriminatory verifiers, scaling in inference-time compute, model capacity and dataset size and does not need humans to verify. Near perfect planning accuracy can be achieved with parallelized global solution evaluator, avoiding formalization of the problem through Mind evolution[1](https://arxiv.org/pdf/2501.09891) strategy with LLM generating/recombining/refining candidate responses.

LLM-based planning [approaches](https://arxiv.org/pdf/2402.02716) include:
- Task decomposition (CoT, ReAct)
- Multi-plan selection (ToT, CoT-SC)
- External planner aided (LLM + PDDL)
- Reflection and Refinement (Reflection, Self-Refine, CRITIC)
- Memory-aided planning (REMEMBER)

These diverse approaches highlight the rapid progress in leveraging LLMs for planning, ranging from enhancing their inherent reasoning abilities to integrating them with external tools and algorithms for more robust and effective decision-making in complex environments.

---

<div id="embodiment">  

</div>

### Embodiment
Real-world physical interaction requires Autonomous Agents capable of making emebodied actions and decisions. 

- [Interactive Agent Foundational Model](https://github.com/tmgthb/Autonomous-Agents#interactiveagent) uses action tokens to enhance grounding with cross-reality data.

---

<div id="tools">  

</div>

### Tool use


---



---

<div id="emotions">  

</div>

### Emotional Intelligence



---

<div id="selflearning">  

</div>

### Self-Recursive Improvement


- ["LMs can Self-Improve"](https://arxiv.org/abs/2403.19154) its own reasoning outputs using techniques such as [CoT](https://github.com/tmgthb/Autonomous-Agents#cot), [Self-Consistency](https://github.com/tmgthb/Autonomous-Agents#selfconsistency) and [In-Context Learning](https://github.com/tmgthb/Autonomous-Agents#multitask) during Inference.
- LLMs can Self-Improve its model weights with: [STaR](https://github.com/tmgthb/Autonomous-Agents#star), where the LLM itself is fine-tuned using correct CoT reasoning.
- [V-STaR](https://github.com/tmgthb/Autonomous-Agents#vstar) improves the STaR-method by making it data efficient: by learning not only from correct, but as well incorrect solutions generated.
- LMs [Recursively Self-Improving (RSI)](https://github.com/tmgthb/Autonomous-Agents#stop) code with [STOP]#stop). Adam Kalai explains insights from this technique in this [lecture about STOP](https://github.com/tmgthb/Autonomous-Agents#stopvideo).
- [LLM Self-Improves its LLM](https://github.com/tmgthb/Autonomous-Agents#restreact) by finetuning with its own synthetic data without human evaluation to imrove mathematical reasoning.
- LLM fine-tuning may be based on [Self-Play](https://github.com/tmgthb/Autonomous-Agents#spin), where the LLM is fine-tuned based on it playing against itself from previous iteration.

---
<div id="brain">  

</div>


### Brain research


[Movie reconstruction from mouse visual cortex activity](https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1)

- Reconstructs ground-truth video using images from mouse brain.


[Brain representation in conscious and unconscious vision](https://www.biorxiv.org/content/10.1101/2024.05.27.596053v1)

- Discovers fronto-parietal cortex is involved in representing unconscious content.


---

<div id="consciousness"> </div>


### Consciousness

There is no single generally agreed definition of Consciousness and I will not try to define it here. 

[Integrated Information Theory (IIT)](https://arxiv.org/abs/1405.7089) and its latest [version 4.0](https://arxiv.org/abs/2212.14787) are one of the key theories existing. This theory includes "Phi", which measures amount of integrated information to quantify level of consciousness of the system. The IIT includes 5 key characteristics:
- Intrinsic
- Composition
- Information
- Integration
- Exclusion

The IIT allows making predictions, which can be tested through experiments and it is not limited to human brain-like consciousness.

[Ilya Sutskever defined, perhaps the first, test-scenario to test, if AI models has consciousness:](https://github.com/tmgthb/Autonomous-Agents#consciousnesstest) for LLMs.

Literature reviews on consciousness:
- [Mathematical Approaches in the Scientific Study of Consciousness](https://jkleiner.de/uploads/preprints/Mathematical%20Approaches%20in%20the%20Scientific%20Study%20of%20Consciousness%20(Preprint,%20Johannes%20Kleiner).pdf)
- [Survey of Consciousness Theory from Computational Perspective](https://arxiv.org/abs/2309.10063)
- [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/abs/2308.08708)


---





<div id="why">  

</div>




<div align="center">

## Why Autonomous Agents work? 

</div>

- [Next sequence prediction](#nextsequenceprediction)
- [Demystifying "Emerging abilities"](#demystifyingemergingabilities)
- [Free energy principle](#freeenergyprinciple)
- [Interpretability](#interpretability)
- [Synthetic data](#syntheticdata)


<div id="nextsequenceprediction">  

</div>

---


### Next sequence prediction

LLMs are trained to predict the next word/token, which leads to: [Multi-task learning](https://github.com/tmgthb/Autonomous-Agents#multitask):
- Backed up by empirical evidence.

The single training objective: "predict next token" results a [Massively Multi-task learning](https://github.com/tmgthb/Autonomous-Agents#extreme).
- "Massively Multi-task learning" results massive amount of new skills learned from a single objective. 

Next sequence prediction algorithm is generic algorithm.
- Next sequence prediction is [generic learning process](https://github.com/tmgthb/Autonomous-Agents#extreme).
- Any "<input, output>"-sequence relationship, can be learned as "next-sequence prediction task".

Information is typically sequential: 
- language is sequence of words,
- DNA is sequence of nucleotides,
- computer programs are sequences of instructions.
- Media: Videos are sequence of images, Music is sequence of notes, image is sequence of pixels and speech is sequence of phonemes.
- Actions: Dance is sequence of movements, day is sequence of events, time is sequence of time steps.
- Concepts about the world: Causality is sequential (cause-effect). Time is sequential(before-after). Life is sequential(parent-child).

Cross-modality Transformers:
- The universal nature of next-sequence prediction is empirically visible in different Transformer models: ViT for Images, Whisper for Audio, SORA for video.

Next sequence prediction is perhaps the most generic single learning objective known to produce intelligent behaviour. 

    I call this surprising, yet unexpected phenomenon as the "Paradox of Lexical Labyrinth".

    Paradox of Lexical Labyrinth:

    The paradoxical phenomenon whereby seemingly simple mechnanism of a next sequence prediction, such as predicting the next word in a     language, gives rise to advanced cognitive skills like profound reasoning capabilities. The labyrinth refers to the vast & complex      landscape of language, characterized by its infinite potential for compressing meaning, expressions and intelligence.

    Teemu Maatta, 07.06.2024

---



<div id="demystifyingemergingabilities">  

</div>


---


### Demystifying Emerging Abilities

[Emerming Abilities](https://github.com/tmgthb/Autonomous-Agents#emerging) refers to ability present in a larger LLM and not in a smaller one.
- The initial definition refers to situation, where emerging abilities have increased so far contiuously as compute is scaled up and more data introduced.

There are +137 known Emerging abilities(increasing).
- Emerging abilities include Emerging Prompting Strategies such as: [CoT](https://github.com/tmgthb/Autonomous-Agents#cot), which was not present in GPT-2 and emerged in GPT-3 model.

Research has [proven](https://arxiv.org/abs/2403.15796) the existing of Emergent abilities from perspective of pre-training loss, even with continuous metrics.

Overall, Emergent abilities are proven to on language models from the perspective of pre-training loss, instead of model/data size.

Emergent abilities suggest that AI models self-organize internal structures to perform tasks to reduce pre-training loss, even without being explicitly programmed for those specific capabilities.


---


<div id="freeenergyprinciple">  

</div>


### Free energy principle

Friston (2010) claims in the [The free energy principle and cognitive agents](https://www.uab.edu/medicine/cinl/images/KFriston_FreeEnergy_BrainTheory.pdf), that biological systems, like human brains, reduce free energy by acting on the world and optimizing their internal states related to perception and action.
- The basic idea is, that biological agents minimize free energy.

Just like human brain mnimizes free energy, the LLMs minimize the prediction error:
- If we give a LLM the training objective of minimizing loss for "next-sequence prediction" and lot of energy/compute and data, then it will self-organize its weights into optimal local order.
- This compression enables LLMs to learn emerging skills beyond merely memorizing the training data.


---

<div id="interpretability">  </div>

### Interpretability

The ability [to extract and directly interpret LLM features](https://arxiv.org/abs/2406.04093) helps to build Autonomous agents, which understand and interact effectively with human language. 

We know as well, that [CLIP-model neurons](https://distill.pub/2021/multimodal-neurons/) can be matched with biological human brain neurons.

Overall, we are now able to both match human and AI model neurons, but as well interpret LLM model features. 


---

<div id="syntheticdata">  </div>

### Synthetic data

Synthetic data is not useful to train even larger AI models, but more importantly to use efficiently scarce-domain data.

The trend of LLMs using [TinyStories](https://github.com/tmgthb/Autonomous-Agents#tinystories) or [Textbook-like datasets with Exercises](https://github.com/tmgthb/Autonomous-Agents#textbookvideo) is known to significantly improve performance of the LLMs. [TinyGSM](https://github.com/tmgthb/Autonomous-Agents#tinygsm) achieved 81.5% accuracy in GSM8K, outperforming significantly larger LLMs. Synthetic data offers in these examples possibility to distill smaller, yet high performing Student LLMs from the Teacher LLM with similar performance level. Secondly, LLMs can be used to generate diverse, yet cheaply available synthetic data to improve reasoning capabilities.
- Autonomous Agents help generate long-range planning and action data withing real-world, which is motivated by enabling finetuning VLMs or LLMs with this data.

---


<div align="center">  

### Related work
Includes list of literature reviews by other authors for quick reference.
</div>

- [A Survey on Large Language Model based Autonomous Agents](https://github.com/tmgthb/Autonomous-Agents#autonomousagentssurvey),
- [LLM Powered Autonomous Agents](https://github.com/tmgthb/Autonomous-Agents#lili),
- [The Rise and Potential of Large Language Model Based Agents: A Survey](https://github.com/tmgthb/Autonomous-Agents#llmagentsurvey),
- [Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives](https://github.com/tmgthb/Autonomous-Agents#humancap),
- [LLMs](https://github.com/tmgthb/Autonomous-Agents#llmsurveymikolov),
- [Unleashing the Power of Graph Learning through LLM-based Autonomous Agents](https://arxiv.org/abs/2309.04565)
- [Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives](https://arxiv.org/abs/2312.11970)
- [Agent AI: Surveying the Horizons of Multimodal Interaction](https://arxiv.org/abs/2401.03568)
- [Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security](https://arxiv.org/abs/2401.05459)
- [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)
- [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196)
- [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282)
- [LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models](https://arxiv.org/abs/2404.01230)
- [A Survey on Large Language Model-Based Game Agents](https://arxiv.org/abs/2404.02039)
- [Comprehensible Artificial Intelligence on Knowledge Graphs: A survey](https://arxiv.org/abs/2404.03499)
- [Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective](https://arxiv.org/abs/2404.06492)
- [System for systematic literature review using multiple AI agents: Concept and an empirical evaluation](https://arxiv.org/abs/2403.08399)
- [Exploring Autonomous Agents through the Lens of Large Language Models: A Review](https://arxiv.org/abs/2404.04442)
- [System for systematic literature review using multiple AI agents: Concept and an empirical evaluation](https://arxiv.org/abs/2403.08399)
- [Real-World Robot Applications of Foundation Models: A Review](https://arxiv.org/abs/2402.05741)
- [Can Large Language Model Agents Simulate Human Trust Behaviors?](https://arxiv.org/abs/2402.04559)
- [Can Generative Agents Predict Emotion?](https://arxiv.org/abs/2402.04232)
- [Large Multimodal Agents: A Survey](https://arxiv.org/abs/2402.15116)
- [Intelligent agents: theory and practice](https://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/ker95.pdf)


---

## Citation


How to cite my work?



```
@misc{MaattaAutonomousAgents2023,
  author = {Teemu Maatta},
  title = {Autonomous Agents},
  year = {2023},
  howpublished = {\url{https://github.com/tmgthb/Autonomous-Agents}},
  note = {Accessed: YYYY-MM-DD}
}

```

---


[Back to top](#topofthepage)
